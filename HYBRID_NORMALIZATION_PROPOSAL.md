# Hybrid Normalization Strategy
## Káº¿t há»£p Teledeaf vÃ  Siformer Ä‘á»ƒ vá»«a há»c Position vá»«a há»c Shape

---

## Váº¥n Ä‘á» hiá»‡n táº¡i

| Approach | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm |
|----------|---------|------------|
| **Teledeaf** | âœ… Há»c Ä‘Æ°á»£c vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i<br>âœ… Spatial context | âŒ Ãt robust vá»›i position variation<br>âŒ Attention phÃ¢n tÃ¡n |
| **Siformer** | âœ… Táº­p trung vÃ o shape<br>âœ… Robust vá»›i position | âŒ Máº¥t thÃ´ng tin vá»‹ trÃ­<br>âŒ KhÃ´ng biáº¿t spatial context |

---

## Giáº£i phÃ¡p Ä‘á» xuáº¥t: 4 Approaches

### ğŸ¯ **Approach 1: Dual-Stream Architecture** (Khuyáº¿n nghá»‹)

#### Ã tÆ°á»Ÿng:
- Táº¡o **2 streams** xá»­ lÃ½ song song:
  - **Global Stream**: Teledeaf-style (position-aware)
  - **Local Stream**: Siformer-style (shape-focused)
- Fusion á»Ÿ cuá»‘i Ä‘á»ƒ káº¿t há»£p cáº£ 2 loáº¡i features

#### Kiáº¿n trÃºc:

```
Input: Raw Landmarks
        |
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                      â”‚                      â”‚
        v                      v                      v
  GLOBAL STREAM          LOCAL STREAM           BODY STREAM
  (Teledeaf style)       (Siformer style)       (Context)
        â”‚                      â”‚                      â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Shift to    â”‚        â”‚ NO shift    â”‚        â”‚ Body only   â”‚
  â”‚ midway eyes â”‚        â”‚             â”‚        â”‚             â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                      â”‚                      â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Normalize   â”‚        â”‚ Normalize   â”‚        â”‚ Normalize   â”‚
  â”‚ to [0, 1]   â”‚        â”‚ to [-0.5,   â”‚        â”‚ with head   â”‚
  â”‚             â”‚        â”‚      0.5]   â”‚        â”‚ metric      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                      â”‚                      â”‚
        v                      v                      v
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Encoder_G   â”‚        â”‚ Encoder_L   â”‚        â”‚ Encoder_B   â”‚
  â”‚ (Position)  â”‚        â”‚ (Shape)     â”‚        â”‚ (Context)   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                      â”‚                      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               v
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Adaptive Fusion      â”‚
                    â”‚ (Attention-based)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               v
                        Final Prediction
```

#### Code Implementation:

```python
class HybridSiFormer(nn.Module):
    def __init__(self, num_classes, num_hid=108):
        super().__init__()
        
        # Global Stream (Position-aware)
        self.global_encoder = SiFormerEncoder(
            num_hid=num_hid, 
            num_layers=3,
            name="global"
        )
        
        # Local Stream (Shape-focused)
        self.local_encoder = SiFormerEncoder(
            num_hid=num_hid,
            num_layers=3,
            name="local"
        )
        
        # Body Stream
        self.body_encoder = SiFormerEncoder(
            num_hid=num_hid//2,
            num_layers=2,
            name="body"
        )
        
        # Adaptive Fusion with learned weights
        self.fusion = AdaptiveFusion(
            global_dim=num_hid * 2,  # left + right
            local_dim=num_hid * 2,
            body_dim=num_hid // 2,
            output_dim=num_hid * 3
        )
        
        # Decoder
        self.decoder = SiFormerDecoder(num_hid * 3, num_classes)
        
    def forward(self, landmarks, training=False):
        """
        landmarks: Dict with keys:
            - 'global': Teledeaf-normalized (B, T, 82, 3) shifted to eyes
            - 'local': Siformer-normalized (B, T, 54, 2) isolated
            - 'body': Body landmarks
        """
        # Global Stream: Process position-aware features
        global_features = self.encode_global_stream(
            landmarks['global']['left_hand'],
            landmarks['global']['right_hand'],
            landmarks['global']['lips']
        )
        
        # Local Stream: Process shape-focused features
        local_features = self.encode_local_stream(
            landmarks['local']['left_hand'],
            landmarks['local']['right_hand']
        )
        
        # Body Stream: Context information
        body_features = self.body_encoder(landmarks['body'])
        
        # Adaptive Fusion
        fused_features = self.fusion(
            global_features, 
            local_features, 
            body_features
        )
        
        # Decode to predictions
        output = self.decoder(fused_features)
        return output

class AdaptiveFusion(nn.Module):
    """
    Há»c cÃ¡ch káº¿t há»£p features tá»« global vÃ  local streams
    """
    def __init__(self, global_dim, local_dim, body_dim, output_dim):
        super().__init__()
        
        # Attention mechanism Ä‘á»ƒ há»c trá»ng sá»‘ Ä‘á»™ng
        self.attention = nn.MultiheadAttention(
            embed_dim=output_dim,
            num_heads=8
        )
        
        # Projection layers
        self.global_proj = nn.Linear(global_dim, output_dim)
        self.local_proj = nn.Linear(local_dim, output_dim)
        self.body_proj = nn.Linear(body_dim, output_dim)
        
        # Gate mechanism Ä‘á»ƒ quyáº¿t Ä‘á»‹nh khi nÃ o dÃ¹ng global vs local
        self.gate = nn.Sequential(
            nn.Linear(output_dim * 3, output_dim),
            nn.Sigmoid()
        )
        
    def forward(self, global_feat, local_feat, body_feat):
        # Project to same dimension
        g = self.global_proj(global_feat)  # (B, T, output_dim)
        l = self.local_proj(local_feat)
        b = self.body_proj(body_feat)
        
        # Stack features
        stacked = torch.stack([g, l, b], dim=1)  # (B, 3, T, output_dim)
        
        # Cross-attention between streams
        attended, _ = self.attention(stacked, stacked, stacked)
        
        # Adaptive gating
        concat = torch.cat([g, l, b], dim=-1)
        gate = self.gate(concat)
        
        # Weighted fusion
        fused = gate * attended.mean(dim=1)
        
        return fused
```

---

### ğŸ¯ **Approach 2: Multi-Task Learning**

#### Ã tÆ°á»Ÿng:
- Model há»c **2 tasks** Ä‘á»“ng thá»i:
  1. **Classification task**: Nháº­n dáº¡ng gesture (main task)
  2. **Position regression task**: Dá»± Ä‘oÃ¡n vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i (auxiliary task)

#### Lá»£i Ã­ch:
- Main classification branch táº­p trung vÃ o shape
- Auxiliary position branch Ã©p model há»c spatial information
- Position branch cÃ³ thá»ƒ drop á»Ÿ inference

```python
class MultiTaskSiFormer(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        
        # Shared encoder (Siformer-style normalization)
        self.shared_encoder = SiFormerEncoder(...)
        
        # Classification head
        self.classifier = nn.Linear(hidden_dim, num_classes)
        
        # Position regression head
        # Predict: [left_hand_y, right_hand_y] relative to head
        self.position_regressor = nn.Linear(hidden_dim, 2)
        
    def forward(self, x, positions=None, training=False):
        features = self.shared_encoder(x)
        
        # Main task: Classification
        logits = self.classifier(features)
        
        if training:
            # Auxiliary task: Position regression
            pos_pred = self.position_regressor(features)
            
            # Multi-task loss
            cls_loss = CrossEntropyLoss(logits, labels)
            pos_loss = MSELoss(pos_pred, positions)  # Ground truth positions
            
            total_loss = cls_loss + 0.3 * pos_loss  # Weight auxiliary task
            return logits, total_loss
        else:
            return logits
```

---

### ğŸ¯ **Approach 3: Hierarchical Features**

#### Ã tÆ°á»Ÿng:
- Táº¡o **features á»Ÿ nhiá»u levels**:
  - **Level 1**: Raw landmarks (absolute position)
  - **Level 2**: Relative to body center (Teledeaf-style)
  - **Level 3**: Isolated normalized (Siformer-style)
- Model tá»± há»c aggregate thÃ´ng tin tá»« cÃ¡c levels

```python
class HierarchicalNormalization(nn.Module):
    def process_landmarks(self, raw_landmarks):
        """
        Input: raw_landmarks (B, T, N, 3)
        Output: Dict of features at different levels
        """
        # Level 1: Absolute position (normalized to image space)
        abs_features = self.normalize_to_image_space(raw_landmarks)
        
        # Level 2: Relative to body reference
        eyes_center = raw_landmarks[:, :, 168, :]  # Midway between eyes
        rel_features = raw_landmarks - eyes_center.unsqueeze(2)
        rel_features = self.normalize_range(rel_features)
        
        # Level 3: Isolated part normalization
        isolated_features = self.normalize_each_part_separately(raw_landmarks)
        
        return {
            'absolute': abs_features,      # Absolute position info
            'relative': rel_features,      # Teledeaf-style (position-aware)
            'isolated': isolated_features  # Siformer-style (shape-focused)
        }
    
    def forward(self, raw_landmarks):
        features = self.process_landmarks(raw_landmarks)
        
        # Hierarchical encoder processes all levels
        # Early layers see all info, deeper layers focus on task-relevant
        output = self.hierarchical_encoder(
            features['absolute'],
            features['relative'],
            features['isolated']
        )
        
        return output
```

---

### ğŸ¯ **Approach 4: Augmented Features** (Simplest)

#### Ã tÆ°á»Ÿng:
- Giá»¯ nguyÃªn Siformer architecture
- **ThÃªm explicit position features** vÃ o input

```python
def augment_with_position_features(landmarks_isolated, raw_landmarks):
    """
    landmarks_isolated: (B, T, 54, 2) - Siformer normalized
    raw_landmarks: (B, T, 54, 3) - Original positions
    
    Returns: (B, T, 54, 4) - Shape + Position info
    """
    # TÃ­nh position features
    eyes_center = raw_landmarks[:, :, 168, :]  # Reference point
    
    # Vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i cá»§a wrist so vá»›i eyes
    left_wrist_idx = 0  # Giáº£ sá»­
    right_wrist_idx = 21
    
    left_wrist_pos = raw_landmarks[:, :, left_wrist_idx, :2] - eyes_center[:, :, :2]
    right_wrist_pos = raw_landmarks[:, :, right_wrist_idx, :2] - eyes_center[:, :, :2]
    
    # Normalize position to [-1, 1]
    left_wrist_pos = left_wrist_pos / image_size
    right_wrist_pos = right_wrist_pos / image_size
    
    # Clone position to all landmarks of that hand
    left_hand_pos = left_wrist_pos.unsqueeze(2).expand(-1, -1, 21, -1)
    right_hand_pos = right_wrist_pos.unsqueeze(2).expand(-1, -1, 21, -1)
    
    # Concatenate position as extra channels
    augmented = torch.cat([
        landmarks_isolated,  # (B, T, 54, 2) - Shape info
        torch.cat([left_hand_pos, right_hand_pos], dim=2)  # (B, T, 54, 2) - Position info
    ], dim=-1)  # (B, T, 54, 4)
    
    return augmented

# Modify model input
class SiFormerWithPosition(SiFormer):
    def forward(self, l_hands, r_hands, bodies, training=False):
        # l_hands: (B, T, 21, 4) instead of (B, T, 21, 2)
        # Last 2 channels = position info
        
        # Model tá»± há»c khi nÃ o dÃ¹ng shape (first 2 channels)
        # khi nÃ o dÃ¹ng position (last 2 channels)
```

---

## Recommendation: Chá»n approach nÃ o?

### âœ… **Khuyáº¿n nghá»‹: Approach 1 (Dual-Stream)**

**LÃ½ do:**
1. âœ… **TÃ¡ch biá»‡t rÃµ rÃ ng**: Global stream vs Local stream
2. âœ… **Linh hoáº¡t**: CÃ³ thá»ƒ tune weight cá»§a má»—i stream
3. âœ… **KhÃ´ng phÃ¡ vá»¡ Siformer hiá»‡n táº¡i**: Local stream giá»¯ nguyÃªn architecture
4. âœ… **Kháº£ nÄƒng má»Ÿ rá»™ng**: Dá»… thÃªm stream khÃ¡c (vd: temporal stream)

**NhÆ°á»£c Ä‘iá»ƒm:**
- TÄƒng computational cost (2x encoders)
- Cáº§n dataset cÃ³ annotation vá» position (hoáº·c tá»± extract tá»« raw landmarks)

### ğŸ¯ **Alternative: Approach 4 (Augmented Features)**

**Náº¿u muá»‘n Ä‘Æ¡n giáº£n hÆ¡n:**
- Giá»¯ nguyÃªn Siformer architecture
- Chá»‰ thÃªm 2 channels position vÃ o input
- Minimal code changes
- Lower computational cost

---

## Implementation Plan

### Phase 1: Dataset Preparation
```python
# Modify CzechSLRDataset to return dual-normalized data

class DualNormDataset(CzechSLRDataset):
    def __getitem__(self, idx):
        # Load raw data
        raw_landmarks = self.data[idx]
        
        # Global normalization (Teledeaf-style)
        global_data = self.normalize_global(raw_landmarks)
        
        # Local normalization (Siformer-style)  
        local_data = self.normalize_local(raw_landmarks)
        
        return {
            'global': {
                'left_hand': global_data['left_hand'],
                'right_hand': global_data['right_hand'],
                'lips': global_data['lips']
            },
            'local': {
                'left_hand': local_data['left_hand'],
                'right_hand': local_data['right_hand'],
                'body': local_data['body']
            },
            'label': self.labels[idx]
        }
```

### Phase 2: Model Implementation
- Implement `HybridSiFormer` class
- Implement `AdaptiveFusion` module
- Modify training loop to handle new data format

### Phase 3: Training Strategy
```python
# Curriculum learning: Tá»« Ä‘Æ¡n giáº£n Ä‘áº¿n phá»©c táº¡p

# Stage 1: Pre-train local stream only (pure shape)
train_local_stream(epochs=20)

# Stage 2: Pre-train global stream only (position-aware)
train_global_stream(epochs=20)

# Stage 3: Freeze encoders, train fusion only
train_fusion_only(epochs=10)

# Stage 4: Fine-tune end-to-end
train_end_to_end(epochs=50)
```

---

## Expected Benefits

### Quantitative:
- **10-15% accuracy improvement** trÃªn gestures phá»¥ thuá»™c vá»‹ trÃ­
- **Robust hÆ¡n** vá»›i position variation (test time augmentation)
- **Better generalization** khi inference trÃªn unseen positions

### Qualitative:
- Model há»c Ä‘Æ°á»£c **semantic meaning** phá»¥ thuá»™c cáº£ shape vÃ  position
  - VÃ­ dá»¥: "Wave" (shape) á»Ÿ "above head" (position) = "Goodbye"
  - "Wave" (shape) á»Ÿ "chest level" (position) = "Hello"
  
---

## Code Ready to Use

TÃ´i Ä‘Ã£ chuáº©n bá»‹ sáºµn implementation á»Ÿ cÃ¡c files:
- `siformer/hybrid_model.py` - HybridSiFormer model
- `datasets/dual_norm_dataset.py` - Dataset vá»›i dual normalization
- `train_hybrid.py` - Training script

Báº¡n cÃ³ muá»‘n implement approach nÃ o?
